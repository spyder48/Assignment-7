# -*- coding: utf-8 -*-
"""Assignment 7

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13VB0lxBs0QipmRGVZj6UAlghOy0Gy8zS
"""

#importing required libraries
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.utils import np_utils

from google.colab import files
uploaded = files.upload()

#Reading the data and converting in to lower form
data = open('shakespeare_input.txt').read().lower()

#Reducing the size of the data to reduce computation time
split_index = int(0.01 * len(data))
data = data[:split_index]

#Getting total no of characters and sorting of data
chars = sorted(list(set(data)))
totalChars = len(data)
print(totalChars)
numberOfUniqueChars = len(chars)

#Representing Characters by a numbers
CharsForids = {char:Id for Id, char in enumerate(chars)}

#This is the opposite to the above
idsForChars = {Id:char for Id, char in enumerate(chars)}

#length of one sequence
numberOfCharsToLearn = 100

#No of steps to be run through the overall dataset
counter = totalChars - numberOfCharsToLearn

counter

#Inpput data
charX = []
#output data
y = []
for i in range(0, counter, 1):
    theInputChars = data[i:i+numberOfCharsToLearn]
    theOutputChars = data[i + numberOfCharsToLearn]
    
    #Appends every 100 chars ids as a list into X
    charX.append([CharsForids[char] for char in theInputChars])
    
    #For every 100 values there is one y value which is the output
    y.append(CharsForids[theOutputChars])

X = np.reshape(charX, (len(charX), numberOfCharsToLearn, 1))

#This is done for normalization
X = X/float(numberOfUniqueChars)


# One hot encoding
y = np_utils.to_categorical(y)
print(y.shape)

#RNN Model


model = Sequential()

model.add(LSTM(512, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))
model.add(Dropout(0.2))

model.add(LSTM(512))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam')
model.fit(X, y, epochs=20, batch_size=128)

#Text Generation

for j in range(0,10):
  
  randomVal = np.random.randint(0, len(charX)-1)
  randomStart = charX[randomVal]
  for i in range(2000):
    x = np.reshape(randomStart, (1, len(randomStart), 1))
    x = x/float(numberOfUniqueChars)
    pred = model.predict(x)
    index = np.argmax(pred)
    randomStart.append(index)
    randomStart = randomStart[1: len(randomStart)]
  print("".join([idsForChars[value] for value in randomStart]))







